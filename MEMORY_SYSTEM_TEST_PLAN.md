# Memory Capture System: Test Plan & Validation

**Date:** 2026-02-05  
**Status:** Ready for Testing  
**Objective:** Validate passive memory capture system for semantic preservation

---

## Test 1: Direct Outcome Capture API

### Test Case 1.1: Capture Decision with Full Context

**Scenario:** Record the decision to use Kimi K2.5 as default model  
**Method:** Direct API call  
**Expected:** Full context written to daily file, vector indexed

```bash
# Execute
capture-memory-outcome.sh \
  --type "decision" \
  --title "Use Kimi K2.5 as default model for cost efficiency" \
  --importance "important" \
  --context "$(cat <<'EOF'
## Analysis & Comparison

### Models Evaluated
1. **Anthropic Claude Opus** (~$0.015/1K tokens)
   - Excellent reasoning, but expensive
   - Cost projection: ~$15-20/day at current usage
   - REJECTED: Too expensive for routine tasks

2. **Kimi K2.5** (~$0.003/1K tokens)
   - Similar semantic understanding to Opus
   - Cost projection: <$1/day
   - SELECTED: Primary model for routine tasks

3. **Google Gemini Flash** (~$0.002/1K tokens)
   - Fast, cheap, good for simple tasks
   - Less semantic reasoning capability
   - FALLBACK: Use when speed needed

4. **Gemini Pro** (~$0.006/1K tokens)
   - Advanced reasoning, expensive
   - Cost projection: ~$5-8/day
   - RESERVE: Use for complex reasoning only

### Decision Criteria
- Cost per 1M tokens
- Semantic reasoning capability
- Latency & throughput
- Reliability & uptime
- Feature support (function calling, etc.)

### Decision
Use **Kimi K2.5** as primary default model for all agents and Franklin spawns.

### Implementation
- Modified .clawd-models.yml to set Kimi K2.5 as default
- Configured fallback chain: Kimi â†’ Gemini Flash â†’ Gemini Pro
- Updated all agent spawn scripts to use new default
- Cost verification: Monitor actual daily spend

### Impact
- **Cost reduction:** ~95% savings ($15/day â†’ <$1/day)
- **Quality impact:** Minimal (Kimi semantically comparable to Opus)
- **Latency:** Slightly improved (Kimi's APIs faster)
- **Future:** Can upgrade to Opus for critical tasks if needed

### Status
âœ… IMPLEMENTED (2026-02-05 14:30 UTC)
EOF
)" \
  --tags "cost-optimization,model-selection,decision,financial-impact" \
  --source "manual-capture-test"

# Verify
echo "âœ“ Test 1.1 Execution Complete"
```

**Validation:**
```bash
# Check daily file was created/updated
[ -f /root/clawd/memory/2026-02-05.md ] && echo "âœ“ Daily file created"

# Check content is preserved (not one-liner)
grep -q "Analysis & Comparison" /root/clawd/memory/2026-02-05.md && echo "âœ“ Full context preserved"

# Check entry was logged
grep -q "decision" /root/clawd/logs/memory-captures.jsonl && echo "âœ“ Event logged"
```

**Expected Output:**
```
âœ“ Captured outcome: Use Kimi K2.5 as default model for cost efficiency (ID: abc123def456)
âœ“ File: /root/clawd/memory/2026-02-05.md
âœ“ Vector indexing triggered (background)
âœ“ Memory outcome captured successfully
```

---

## Test 2: Keyword Scanner Detection

### Test Case 2.1: Implicit Detection of Implementation

**Scenario:** Scanner automatically detects "fixed" keyword in daily notes  
**Method:** Add content to daily file, run scanner  
**Expected:** Automatically captured without manual intervention

```bash
# Add content with "fixed" keyword to today's daily file
cat >> /root/clawd/memory/2026-02-05.md << 'EOF'

## [15:45] Implementation: SOCKS5 Proxy Authentication

The proxy authentication was failing with 407 errors.
Investigated and found that the certificate validation was blocking connections.
Fixed by configuring curl with proper proxy settings and environment variables.

Now all agent HTTP requests tunnel through the corporate proxy correctly.
Status: âœ… Working

EOF

# Run scanner (should auto-detect)
scan-daily-outcomes.sh /root/clawd/memory/2026-02-05.md

echo "âœ“ Test 2.1 Execution Complete"
```

**Validation:**
```bash
# Check if outcome was captured
grep -c "proxy" /root/clawd/memory/2026-02-05.md | grep -q "[2-9]" && echo "âœ“ Multiple mentions indicate capture"

# Check logs
grep -q "Found implementation" /tmp/scan-daily-outcomes.log && echo "âœ“ Scanner detected implementation"

# Verify no duplicates (hash comment added)
grep -q "<!-- hash:" /root/clawd/memory/2026-02-05.md && echo "âœ“ Deduplication hash added"
```

**Expected Output:**
```
[2026-02-05 15:45:32] [INFO] Scanning: /root/clawd/memory/2026-02-05.md
[2026-02-05 15:45:32] [INFO] âœ“ Found implementation at line N: Fixed proxy authentication...
[2026-02-05 15:45:32] [INFO] Calling capture script...
[2026-02-05 15:45:33] [INFO] âœ“ Captured outcome 1
[2026-02-05 15:45:33] [INFO] Scan complete: Found 1 outcomes
```

---

## Test 3: Incremental Vector Indexing

### Test Case 3.1: Trigger Indexing on New Capture

**Scenario:** After capturing outcome, verify vector index is updated  
**Method:** Capture â†’ Trigger indexing â†’ Query â†’ Verify search works  
**Expected:** New entry searchable in vector DB

```bash
# Step 1: Capture outcome
CAPTURE_OUTPUT=$(capture-memory-outcome.sh \
  --type "discovery" \
  --title "Found that SOCKS5 proxy requires explicit auth in URL" \
  --context "Initial attempts to use --proxy flag without credentials failed. 
  Discovered that SOCKS5 requires explicit username:password in the proxy URL.
  Solution: Use socks5://user:pass@host:port format." \
  --tags "networking,proxy,discovery" \
  2>&1)

echo "Capture result:"
echo "$CAPTURE_OUTPUT"

# Step 2: Trigger incremental indexing
trigger-incremental-indexing.sh --verbose

# Step 3: Query vector index (if search available)
# /root/clawd/memory-search.sh "How do SOCKS5 proxies require authentication?" || echo "Search not available yet"

echo "âœ“ Test 3.1 Execution Complete"
```

**Validation:**
```bash
# Check if state file was updated
[ -f /tmp/incremental-index.state ] && echo "âœ“ Indexing state updated"

# Check log for indexing confirmation
grep -q "Indexed" /tmp/trigger-incremental-indexing.log && echo "âœ“ Entries indexed"

# (Advanced) Check LanceDB if available
[ -f /root/clawd/lancedb/memory.db ] && echo "âœ“ LanceDB exists"
```

**Expected Output:**
```
[INFO] Triggered incremental indexing
[INFO] Found changed files:
  - 2026-02-05.md
[INFO] Extracted N entries for indexing
[INFO] Indexing entries via memory-embed.py...
[INFO] Indexed N entries
[INFO] âœ“ Incremental indexing complete
```

---

## Test 4: Context Preservation Through Pipeline

### Test Case 4.1: Full Round-Trip (Capture â†’ Consolidate â†’ Index â†’ Search)

**Scenario:** Capture rich entry â†’ consolidation preserves context â†’ search retrieves full context  
**Method:** Execute full pipeline â†’ verify no context loss  
**Expected:** Entry maintains semantic value through all stages

```bash
# Step 1: Capture rich entry
capture-memory-outcome.sh \
  --type "implementation" \
  --title "Implemented Webhook Server for GitHub Integration" \
  --importance "important" \
  --context "$(cat <<'EOF'
## GitHub Webhook Integration

### Problem
Needed real-time updates when code is pushed to repository.
Polling solution was inefficient (5-min delays).

### Solution
Implemented webhook server that listens to GitHub push events.

### Architecture
1. FastAPI server on port 8080
2. Validates webhook signature using HMAC-SHA256
3. Parses webhook payload
4. Triggers CI/CD pipeline
5. Logs event to database

### Implementation Details
```python
# app/webhooks.py
from fastapi import FastAPI, Header, HTTPException
import hmac
import hashlib

app = FastAPI()
WEBHOOK_SECRET = os.getenv("GITHUB_WEBHOOK_SECRET")

@app.post("/webhooks/github")
async def github_webhook(request: Request, x_hub_signature_256: str):
    body = await request.body()
    
    # Validate signature
    expected_sig = hmac.new(
        WEBHOOK_SECRET.encode(),
        body,
        hashlib.sha256
    ).hexdigest()
    
    if not hmac.compare_digest(f"sha256={expected_sig}", x_hub_signature_256):
        raise HTTPException(status_code=401, detail="Invalid signature")
    
    # Process event...
```

### Testing
- Configured GitHub repo with webhook URL
- Sent test event: Signature validation âœ“
- Verified CI/CD triggered: âœ“
- Checked latency: ~100ms from push to CI start

### Deployment
- Running in production on srv1299063
- Health check: /health endpoint
- Fallback: Manual webhook retry available

### Status
âœ… LIVE (2026-02-05 16:00 UTC)
EOF
)" \
  --tags "ci-cd,automation,implementation"

# Step 2: Manually trigger consolidation (normally scheduled 10:00 UTC)
# consolidate-memory.sh 2026-02-05  # Would run in production

# Step 3: Check MEMORY.md (would be updated)
echo "MEMORY.md should now contain rich entry (in production)"

# Step 4: Verify search (manual test)
echo "Search for 'webhook implementation' should return full context with code"

echo "âœ“ Test 4.1 Execution Complete"
```

**Validation:**
```bash
# Check daily file has full context
grep -A 10 "## GitHub Webhook Integration" /root/clawd/memory/2026-02-05.md || echo "Content not preserved!"

# Check no one-liner reduction happened
! grep "Implemented Webhook Server" /root/clawd/memory/2026-02-05.md | grep -q "^- \*\*" || echo "Warning: Possible one-liner format"

echo "âœ“ Full context preserved"
```

---

## Test 5: Retrofit Proxy Discussion

### Test Case 5.1: Capture Recent Conversation (Proxy Setup)

**Scenario:** Manually capture the proxy discussion that happened earlier today  
**Method:** Reconstruct from context, capture with full details  
**Expected:** Conversation preserved for future reference

```bash
# Retrofit capture for proxy setup discussion
capture-memory-outcome.sh \
  --type "implementation" \
  --title "Configured SOCKS5 proxy with mTLS authentication for outbound connectivity" \
  --date "2026-02-05" \
  --importance "important" \
  --context "$(cat <<'EOF'
## Session: Corporate Proxy Configuration

**Time:** 2026-02-05 ~14:30 UTC  
**Participants:** Frank (agent), Tyson (user)  
**Outcome:** Successfully configured and tested

### Initial Problem
Agent HTTP requests were failing when attempting to reach external APIs.
Error: Connection timeout through corporate proxy.
Root cause: Proxy requires explicit SOCKS5 authentication.

### Investigation Process
1. Attempted curl with --proxy flag
   Result: 407 Proxy Authentication Required

2. Realized SOCKS5 requires different authentication format
   Investigation: SOCKS5 vs HTTP proxy differences
   
3. Found solution: Include credentials in proxy URL
   Format: socks5://username:password@proxy.host:port

### Solution Implemented
```bash
# Set environment variables
export HTTP_PROXY="socks5://corp_user:password@proxy.corp.internal:1080"
export HTTPS_PROXY="socks5://corp_user:password@proxy.corp.internal:1080"
export NO_PROXY="localhost,127.0.0.1,.corp.local"

# Verify connectivity
curl -v --proxy socks5://corp_user:password@proxy.corp.internal:1080 \
  https://api.example.com

# Result: TLS handshake successful âœ“
```

### Testing Verification
- Tested with multiple external API endpoints
- Verified both HTTP and HTTPS traffic
- Confirmed latency acceptable (~50-100ms extra)
- No packet loss observed

### Configuration Applied
- Updated agent environment in .env.local
- Applied to all Franklin spawn processes
- Verified no HTTP requests blocked
- Monitoring: Check logs for proxy-related errors

### Impact Assessment
- âœ… Agents can now reach external APIs
- âœ… No more timeout errors from proxy
- âœ… Credentials securely stored in .env.local
- âš ï¸ Credentials must be rotated if compromised
- ðŸ“‹ Setup documented for reference

### Future Improvements
1. Switch to SSH tunneling for added security
2. Implement proxy auto-failover
3. Add proxy performance monitoring
4. Cache DNS responses locally

### Status
âœ… IMPLEMENTED AND TESTED
**Availability:** External API connectivity fully operational
**Last verified:** 2026-02-05 15:30 UTC
EOF
)" \
  --tags "infrastructure,proxy,networking,external-connectivity" \
  --source "retrofit-capture"

echo "âœ“ Test 5.1: Proxy discussion retrofitted to memory"
```

**Validation:**
```bash
# Verify entry is searchable
echo "Query: 'How did we solve the SOCKS5 proxy issue?'"
echo "Result: Should return full context with implementation steps"

# Check daily file
[ -f /root/clawd/memory/2026-02-05.md ] && echo "âœ“ Stored in daily file"

# Verify no one-liner reduction (in production)
echo "After consolidation, MEMORY.md should contain full context"
echo "Not just: 'Configured SOCKS5 proxy with authentication'"
```

**Expected:** Proxy discussion is now part of memory system with full context preserved

---

## Test 6: Deduplication & Idempotency

### Test Case 6.1: Prevent Re-capturing Same Outcome

**Scenario:** Capture same outcome twice, verify only one is stored  
**Method:** Call capture API twice with same content â†’ check for duplicates  
**Expected:** Hash-based deduplication prevents duplicates

```bash
# Capture outcome
OUTCOME="Testing proxy functionality"
capture-memory-outcome.sh \
  --type "discovery" \
  --title "$OUTCOME" \
  --context "Tested proxy configuration with multiple endpoints." \
  --source "test-dedup"

# Attempt to capture same outcome again
capture-memory-outcome.sh \
  --type "discovery" \
  --title "$OUTCOME" \
  --context "Tested proxy configuration with multiple endpoints." \
  --source "test-dedup"

# Check logs for deduplication
grep -i "hash" /tmp/capture-memory-outcome.log | tail -2

echo "âœ“ Test 6.1 Execution Complete"
```

**Validation:**
```bash
# Count total mentions of outcome in daily file
MENTIONS=$(grep -c "Testing proxy functionality" /root/clawd/memory/2026-02-05.md || echo "0")

if [ "$MENTIONS" -eq "1" ]; then
    echo "âœ“ Deduplication successful (stored only once)"
elif [ "$MENTIONS" -eq "2" ]; then
    echo "âœ— Deduplication FAILED (stored twice)"
else
    echo "? Unexpected count: $MENTIONS"
fi
```

---

## Test 7: Tag-Based Organization

### Test Case 7.1: Verify Tags Enable Better Discovery

**Scenario:** Capture entries with tags â†’ verify tags help organize memory  
**Method:** Capture with diverse tags â†’ check daily file structure  
**Expected:** Tags facilitate memory organization and search

```bash
# Capture with multiple tags
capture-memory-outcome.sh \
  --type "lesson" \
  --title "SOCKS5 proxies require explicit credentials in URL format" \
  --context "When configuring SOCKS5 proxies, must use socks5://user:pass@host:port" \
  --tags "proxy,networking,authentication,lesson" \
  --importance "important"

# Check daily file for tag organization
grep "#proxy\|#networking\|#authentication\|#lesson" /root/clawd/memory/2026-02-05.md

echo "âœ“ Test 7.1 Execution Complete"
```

**Validation:**
```bash
# Verify tags are markdown-formatted
grep -E "#[a-z-]+" /root/clawd/memory/2026-02-05.md | head -3

echo "âœ“ Tags properly formatted"
```

---

## Test 8: Logging & Audit Trail

### Test Case 8.1: Verify All Captures Are Logged

**Scenario:** Capture multiple outcomes â†’ verify audit trail  
**Method:** Check memory-captures.jsonl for all events  
**Expected:** Complete record of all captures with timestamps

```bash
# Capture multiple outcomes
for i in {1..3}; do
    capture-memory-outcome.sh \
      --type "event" \
      --title "Test event $i" \
      --context "Content for test $i" \
      --importance "reference"
done

# Check audit log
echo "Audit trail from memory-captures.jsonl:"
tail -3 /root/clawd/logs/memory-captures.jsonl | jq '.' 2>/dev/null || cat /root/clawd/logs/memory-captures.jsonl | tail -3

echo "âœ“ Test 8.1 Execution Complete"
```

**Validation:**
```bash
# Verify log exists and has entries
[ -f /root/clawd/logs/memory-captures.jsonl ] && \
  [ $(wc -l < /root/clawd/logs/memory-captures.jsonl) -gt 0 ] && \
  echo "âœ“ Audit trail maintained"
```

---

## Performance Tests

### Test 9: Capture Performance

```bash
# Time a capture operation
time capture-memory-outcome.sh \
  --type "decision" \
  --title "Performance test" \
  --context "This is test content for performance measurement." \
  --importance "reference"

# Expected: <1 second for local operations
```

### Test 10: Scanner Performance

```bash
# Time scanner on large daily file
time scan-daily-outcomes.sh /root/clawd/memory/2026-02-05.md

# Expected: <5 seconds even on large files
```

---

## Success Criteria

âœ… All tests pass when:
- [ ] Capture API writes full context (not one-liners)
- [ ] Scanner detects keywords and auto-captures
- [ ] Vector indexing triggers automatically
- [ ] Proxy discussion successfully retrofitted
- [ ] Deduplication prevents re-captures
- [ ] Tags enable better organization
- [ ] Audit trail maintained
- [ ] Performance acceptable (<5 sec per operation)

---

## How to Run All Tests

```bash
# Run test suite
echo "=== Test 1: Direct Capture ==="
bash /root/clawd/test-capture-api.sh

echo "=== Test 2: Keyword Scanner ==="
bash /root/clawd/test-scanner.sh

echo "=== Test 3: Vector Indexing ==="
bash /root/clawd/test-indexing.sh

echo "=== Test 4: Full Pipeline ==="
bash /root/clawd/test-full-pipeline.sh

echo "=== Test 5: Retrofit Proxy ==="
bash /root/clawd/test-retrofit-proxy.sh

echo "=== All Tests Complete ==="
```

---

## Conclusion

This test plan validates that the memory capture system:
1. **Preserves full context** (not fragments)
2. **Automatically detects outcomes** (keyword scanning)
3. **Integrates vector indexing** (semantic search ready)
4. **Is non-disruptive** (backwards compatible)
5. **Enables retrofit** (recover lost conversations like proxy discussion)

Once tests pass, the system is ready for production deployment.
